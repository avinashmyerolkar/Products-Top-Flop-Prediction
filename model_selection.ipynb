{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fb0375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fd878b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>main_promotion</th>\n",
       "      <th>color</th>\n",
       "      <th>stars</th>\n",
       "      <th>success_indicator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category  main_promotion  color  stars  success_indicator\n",
       "0         5               0      3    1.0                  0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here df we will be using which is already encoded during explaining EDA and feature engineering part , in EDA file\n",
    "df=pd.read_csv('train_df.csv')\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80617620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler  # as distance based algorithem is there\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ec9551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')  #to do away with unwanted or unuseful warnings which makes code lengthy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb342ee",
   "metadata": {},
   "source": [
    "# Model_selection_Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b5a7827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'classifier': LogisticRegression(C=0.1), 'classifier__C': 0.1}\n",
      "Best Score: 0.7579559467526525\n",
      "Accuracy: 0.7512054001928641\n",
      "Precision: 0.7098919368246052\n",
      "Recall: 0.8364348677766895\n",
      "F1 Score: 0.7679856115107914\n",
      "***************************************************\n",
      "Best Parameters: {'classifier': RandomForestClassifier(n_estimators=200), 'classifier__n_estimators': 200}\n",
      "Best Score: 0.8294354270608656\n",
      "Accuracy: 0.8278688524590164\n",
      "Precision: 0.7938053097345132\n",
      "Recall: 0.8785504407443683\n",
      "F1 Score: 0.8340306834030683\n",
      "***************************************************\n",
      "Best Parameters: {'classifier': MLPClassifier(hidden_layer_sizes=(50, 50)), 'classifier__activation': 'relu', 'classifier__hidden_layer_sizes': (50, 50), 'classifier__solver': 'adam'}\n",
      "Best Score: 0.8105107591305547\n",
      "Accuracy: 0.8211186113789778\n",
      "Precision: 0.7850877192982456\n",
      "Recall: 0.8765915768854065\n",
      "F1 Score: 0.828320222119389\n",
      "***************************************************\n",
      "Best Model: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('classifier', RandomForestClassifier(n_estimators=200))])\n",
      "Best Score: 0.8294354270608656\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = df\n",
    "x=df.drop('success_indicator',axis=1)\n",
    "y=df['success_indicator']\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=77)\n",
    "\n",
    "pipeline = Pipeline([('scaler', StandardScaler()),('classifier', None) ])\n",
    "\n",
    "# Adding models to the param_grids list so to compare their accuracy with same data but different hyperparamere\n",
    "param_grids = [\n",
    "    {'classifier': [LogisticRegression()],    \n",
    "     'classifier__C': [0.1, 1, 10]},\n",
    "    {'classifier': [RandomForestClassifier()],\n",
    "     'classifier__n_estimators': [100, 200, 300]},  #number of base model used to predict output\n",
    "    {'classifier': [MLPClassifier()],\n",
    "     'classifier__hidden_layer_sizes': [(100,), (50, 50), (25, 25, 25)],\n",
    "     'classifier__activation': ['relu', 'tanh'],  # these will be our activation function\n",
    "     'classifier__solver': ['adam']}              # adam will be optimiazer for ANN\n",
    "]\n",
    "\n",
    "best_model = None\n",
    "best_score = 0\n",
    "\n",
    "for entity in param_grids:\n",
    "    gs = GridSearchCV(pipeline, entity, cv=5, scoring='accuracy')  # cross validation in 5 , while measure of comparison \n",
    "    gs.fit(x_train, y_train)                                       # is acccuracy , we may use random search cv also if \n",
    "                                                                    # data is more\n",
    "\n",
    "    y_pred = gs.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(\"Best Parameters:\", gs.best_params_)\n",
    "    print(\"Best Score:\", gs.best_score_)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"***************************************************\")\n",
    "\n",
    "    if gs.best_score_ > best_score:\n",
    "        best_model = gs.best_estimator_\n",
    "        best_score = gs.best_score_\n",
    "\n",
    "print(\"Best Model:\", best_model)\n",
    "print(\"Best Score:\", best_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013632bb",
   "metadata": {},
   "source": [
    "# Random forest classifier will be the model which we will be choosing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6066326a",
   "metadata": {},
   "source": [
    "# Reasons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fbdf0f",
   "metadata": {},
   "source": [
    "1.Accuracy perspective : Among majority of the models which we have used for modelling the data we getting high accuracy on random forest classifier.\n",
    "\n",
    "2.Principle of working of random-forest classifier: Random Forest is an ensemble learning method that combines multiple decision trees to make predictions. And as we are taking decision from multiple models problem of overfitting reduces along with increase in accuracy.\n",
    "\n",
    "3.Robustness to outliers: As Random forest is ruled based algorithem unlike logistic regression which is distance based it is generally robust to outliers\n",
    "\n",
    "4.Easy to interprete and understand : Random forest is based on wisdom of group principle and hence easy to understand along with ease in interpretation of hyperparametre for tuning model.\n",
    "\n",
    "These are the main 4 reason which we should take into consideration while choosing Random-Forest classifier as our model for training data\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
